# Integration Plan: Vision-Based Medical Figure Generator (Revised)

> **Revision 2** — Updated to use **vision-based label placement** instead of bounding-box entity layout.
> The original DiagrammerGPT multi-entity bounding box approach doesn't fit medical anatomy diagrams
> (single unified illustration with sub-region labels, not multiple discrete objects).
> This revision adds a **Vision Label Placer** stage that uses GPT-4o/Gemini Vision to analyze the
> generated image and identify precise anatomical part coordinates.

## Executive Summary

**Problem:** Our current pipeline generates images via DALL-E / Gemini Imagen, then tries to vectorize (PNG → SVG) via PyTorch-SVGRender LIVE. The vectorizer takes 30+ minutes on consumer GPU and still produces imprecise annotations because the LLM guesses label positions blindly (without seeing the image).

**Solution:** A 4-stage pipeline combining DiagrammerGPT's planning ideas with **vision-model-based label placement**:
1. **LLM plans** the drawing prompt + label list + spatial hints (simplified from DiagrammerGPT — no bounding boxes for single-object anatomy)
2. **Image gen API** creates the raster image with spatial hints to leave label space
3. **Vision model analyzes the actual image** to identify precise pixel coordinates of each anatomical part (the key innovation vs. blind guessing)
4. **Pillow/SVG renders labels** programmatically at precise positions with user-chosen annotation style

---

## 1. What We're Taking From DiagrammerGPT

### 1.1 Core Ideas We Adopt

| DiagrammerGPT Concept | Our Adaptation |
|---|---|
| **Diagram Plan** — LLM generates entities, relationships, bounding boxes in [x,y,w,h] normalized coords (0-100) | Same. We use Gemini/GPT-4o to generate a structured plan with entity bounding boxes, label positions, and relationships |
| **Planner-Auditor feedback loop** — A second LLM pass checks the plan for errors (overlaps, missing entities, broken relationships) and the planner fixes them | Same. We run 1-3 auditor iterations to catch layout errors before image generation |
| **Explicit text rendering via Pillow** — Text labels are NOT generated by the image model; they're rendered programmatically after image generation | Same. We render labels using Pillow (on PNG) or SVG `<text>` elements, with user-customizable styles |
| **Normalized coordinate system** [0-100] | Same. LLM plans in a 100×100 coordinate space; we scale to actual image dimensions |

### 1.2 What We DON'T Take

| DiagrammerGPT Component | Why We Skip It |
|---|---|
| **DiagramGLIGEN model** — Custom layout-guided diffusion model trained on AI2D-Caption dataset | We use DALL-E 3 / Gemini Imagen / Nano Banana Pro instead. These are better general-purpose generators and require no local GPU for inference |
| **AI2D-Caption dataset** | Not needed — we don't train a custom model |
| **Local Stable Diffusion inference** | We use cloud APIs instead |
| **PyTorch-SVGRender LIVE vectorizer** | **REMOVED** — This was the 30-min bottleneck. We render annotations directly on raster images or build simple SVGs from scratch |

---

## 2. New Architecture

### 2.1 Pipeline Overview (Before vs After)

**BEFORE (current, broken):**
```
User → [Stage 1: LLM structures prompt] → [Stage 2: Image Gen API] → [Stage 3: LIVE Vectorizer (30min!)] → [Stage 4: Blind LLM annotation] → [Stage 5: Refine]
```

**AFTER (new vision-based):**
```
User → [Stage 1: Diagram Planner + Auditor] → [Stage 2: Image Gen API] → [Stage 3: Vision Label Placer (GPT-4o/Gemini sees the image!)] → [Stage 4: Label Renderer (Pillow/SVG)] → [Stage 5: Refine]
```

### 2.2 Why Vision-Based Label Placement?

The original DiagrammerGPT approach uses bounding boxes for individual entities (I0, I1, T0, T1...) with explicit [x,y,w,h] coordinates. This works well for **multi-object diagrams** (circuit diagrams, flowcharts, life cycles) where each entity is a discrete visual object.

**But for medical anatomy diagrams** (our primary use case), the illustration is a **single unified object** (e.g., "a human hand") with multiple sub-region labels pointing to parts of that one object. You can't meaningfully assign separate bounding boxes to "phalanges" vs "metacarpals" — they're all part of the same hand illustration.

**Vision-based label placement solves this** by:
1. Generating the image first (as a single unified illustration)
2. Showing the actual image to a vision model (GPT-4o / Gemini Vision)
3. Asking it to identify precisely WHERE each anatomical part is in the image
4. Then rendering labels at those precise locations

### 2.3 Detailed Stage Design

---

#### Stage 1: Diagram Planner (`pipeline/diagram_planner.py`) — NEW

**Replaces:** `prompt_structurer.py`

**Input:** User message (e.g., "draw a human hand anatomy")

**Output:** `DiagramPlan` — a simplified structured dict (no bounding boxes needed for anatomy)

```json
{
  "drawing_prompt": "Scientific medical textbook illustration of human hand skeletal anatomy...",
  "labels": ["Distal Phalanges", "Middle Phalanges", "Proximal Phalanges", "Metacarpals", "Carpals"],
  "label_side": "right",
  "style": "colored_diagram",
  "description": "Dorsal view of human hand skeletal anatomy showing all 27 bones.",
  "diagram_type": "anatomy"
}
```

**Key simplification vs original DiagrammerGPT plan:**
- NO entity bounding boxes (not needed for single-object anatomy)
- NO relationships / leader_line specs (the vision model will determine these)
- Just: drawing prompt + label list + spatial hint + diagram type

**Includes Auditor Loop:** A second LLM pass checks the plan for completeness (missing labels, bad spatial hints) and the planner fixes issues. Max 2 iterations.

---

#### Stage 2: Image Generator (`pipeline/image_generator.py`) — MODIFIED

**Changes from current:** The prompt now includes a **spatial hint** (from the planner's `drawing_prompt`) telling the image model to leave space for labels.

**Input:** `DiagramPlan` (specifically the `drawing_prompt`)

**Output:** Raster PNG (1024×1024)

**No changes to API call logic** — the planner's `drawing_prompt` already contains spatial hints like "Position the main illustration in the left 70% of the canvas."

---

#### Stage 3: Vision Label Placer (`pipeline/vision_label_placer.py`) — ★ NEW KEY STAGE ★

**This is the critical innovation.** Instead of guessing label positions blindly (old annotator) or pre-planning bounding boxes (DiagrammerGPT), we **show the actual generated image to a vision model** and ask it to identify where each anatomical part is.

**Input:** Raster PNG + label list + description

**Output:** List of precise pixel coordinates: `[{label, point_x, point_y}, ...]`

**How it works:**
1. Load the generated raster image
2. Send it to GPT-4o (vision) or Gemini 2.0 Flash (vision) along with the label list
3. The vision model analyzes the image and returns exact pixel coordinates for each anatomical structure
4. Coordinates are validated and clamped to image bounds

**Why this works for medical anatomy:**
- The vision model can see the actual image content
- It knows where the phalanges are vs. where the metacarpals are
- No bounding boxes needed — just point coordinates (x, y) for each part
- Works for any single-object anatomy diagram (hand, brain, heart, neuron, eye, etc.)

---

#### Stage 4: Label Renderer (`pipeline/label_renderer.py`) — NEW, REPLACES `vectorizer.py` + `annotator.py`

Takes the raster image + label positions from the Vision Label Placer and renders clean text labels on top.

**Input:** Raster PNG + label positions (from Stage 3) + annotation style

**Output:** Annotated PNG + annotated SVG (raster embedded + vector label overlays)

**Label layout algorithm:**
1. Sort labels by their y-coordinate (top-to-bottom)
2. Position label text on the designated side (usually right)
3. Avoid vertical overlaps by pushing labels down
4. Draw leader lines from label text to anatomical point
5. Apply the user's chosen annotation style

**Why Pillow for PNG output:**
- Crisp, readable text rendering with TrueType fonts
- No GPU needed, runs in milliseconds
- Full control over fonts, sizes, colors, backgrounds

**Why also SVG output:**
- Embeds the raster as `<image>` inside an SVG container
- Labels are real `<text>` elements — editable, searchable, scalable
- Leader lines are `<line>` elements

---

#### Stage 5: Refiner (`pipeline/refiner.py`) — SIMPLIFIED

The refiner now works with the `DiagramPlan` + label positions:

- **Label text edits** → modify the plan's label list, re-run vision placer + renderer
- **Style change** → re-render with new style (instant, no API call)
- **Add/remove label** → modify label list, re-run vision placer + renderer
- **Regenerate** → re-run full pipeline with modified prompt

This is much faster than the current approach because most refinements only need label re-rendering (milliseconds).

---

## 3. UI Changes

### 3.1 New Options in the Chat UI

```
┌─────────────────────────────────────────────┐
│ LLM: [Gemini ▼]  Image: [DALL-E 3 ▼]       │
│                                              │
│ Label Style: [Boxed Labels ▼]               │
│   ○ Plain Text                               │
│   ○ Boxed Labels        ← default           │
│   ○ Numbered Callouts                        │
│   ○ Color-Coded                              │
│   ○ Minimal Lines                            │
│   ○ Textbook Style                           │
│                                              │
│ □ Skip annotation                            │
│ □ Show diagram plan (debug)                  │
└─────────────────────────────────────────────┘
```

### 3.2 Preview Panel Enhancement

- **Tab 1: Annotated Image** (annotated PNG)
- **Tab 2: Editable SVG** (raster + vector labels)
- **Tab 3: Diagram Plan** (optional debug view showing the entity layout as a schematic)

### 3.3 Removed

- "Skip vectorization" checkbox (no longer needed — vectorizer is gone)

---

## 4. File Changes Summary

### Files to CREATE

| File | Purpose |
|------|---------|
| `pipeline/diagram_planner.py` | Stage 1: LLM diagram plan generation + auditor loop |
| `pipeline/vision_label_placer.py` | Stage 3: Vision model identifies anatomical part coordinates |
| `pipeline/label_renderer.py` | Stage 4: Pillow/SVG text rendering with 6 annotation styles |

### Files to MODIFY

| File | Changes |
|------|---------|
| `config.py` | Remove vectorizer settings. Add `DEFAULT_LABEL_STYLE`, `AUDITOR_MAX_ITERATIONS`, `FONT_PATH`. Remove `SVGRENDER_ROOT`, `PYTHON_EXE`. |
| `app.py` | Replace pipeline stages. Accept `label_style` from frontend. Remove vectorizer call. Wire up vision placer + label renderer. |
| `pipeline/image_generator.py` | Remove unused `IMAGE_SIZE` import. Prompt construction unchanged (planner handles spatial hints). |
| `pipeline/refiner.py` | Refine against `DiagramPlan` instead of raw SVG. Label edits = modify plan + re-render. |
| `templates/index.html` | Add label style selector. Remove "skip vectorization" checkbox. Add diagram plan debug tab. |
| `requirements.txt` | Remove PyTorch-SVGRender deps. Keep `Pillow` (already there). |

### Files to DELETE / DEPRECATE

| File | Reason |
|------|--------|
| `pipeline/vectorizer.py` | LIVE vectorizer removed entirely |
| `pipeline/annotator.py` | Replaced by `label_renderer.py` |
| `pipeline/prompt_structurer.py` | Replaced by `diagram_planner.py` |

---

## 5. Diagram Plan Format (Full Spec)

The central data structure that flows through the pipeline:

```json
{
  "drawing_prompt": "Scientific medical textbook illustration of human hand skeletal anatomy...",
  "labels": ["Distal Phalanges", "Middle Phalanges", "Proximal Phalanges", "Metacarpals", "Carpals"],
  "label_side": "right",
  "style": "colored_diagram",
  "description": "Dorsal view of human hand skeletal anatomy showing all 27 bones.",
  "diagram_type": "anatomy"
}
```

### Key differences from DiagrammerGPT's entity format:
1. **No `entities` with bounding boxes** — not needed for single-object anatomy
2. **No `relationships`** — the vision model determines spatial relationships by seeing the image
3. **`label_side`** hint replaces complex layout specifications
4. **`diagram_type`** distinguishes "anatomy" (single object) from "relational" (multiple objects)

---

## 6. Implementation Order

### Phase 1: Core Pipeline
1. Create `pipeline/diagram_planner.py` with planner + auditor
2. Create `pipeline/vision_label_placer.py` — vision-based anatomical point detection
3. Create `pipeline/label_renderer.py` with all 6 styles
4. Modify `pipeline/image_generator.py` (minor — remove unused import)
5. Update `config.py` (remove vectorizer config, add label/font settings)
6. Rewrite `app.py` to wire up new 4-stage pipeline
7. Delete `pipeline/vectorizer.py`, `pipeline/prompt_structurer.py`, `pipeline/annotator.py`

### Phase 2: UI + Refiner
8. Update `templates/index.html` — label style selector, remove vectorize checkbox
9. Rewrite `pipeline/refiner.py` to work with plan + label positions
10. End-to-end testing

---

## 7. Performance Comparison

| Metric | Old (LIVE Vectorizer) | New (Vision-Based) |
|--------|----------------------|------------------------------|
| Stage 1 (prompt/plan) | ~3s | ~5s (plan is more detailed) |
| Stage 1b (auditor) | N/A | ~3-8s (1-2 LLM calls) |
| Stage 2 (image gen) | ~10s | ~10s (same) |
| Stage 3 (vision placer) | N/A | ~3-5s (one vision API call) |
| Stage 3→4 (vectorize→render) | **30+ min (FAILED)** | **<1 second (Pillow)** |
| **Total** | **30+ min (timeout)** | **~25-30 seconds** |

The new pipeline is **~60x faster** because we replaced the GPU-bound vectorizer with API calls + Pillow rendering.

---

## 8. Annotation Style Details

### 8.1 `plain_text`
```
        Phalanges ──────── ●
```
- Simple text with a thin leader line
- Small dot at the pointed location
- Clean, minimal look

### 8.2 `boxed_text` (default)
```
    ┌──────────────┐
    │  Phalanges   │────── ●
    └──────────────┘
```
- Text inside a rounded rectangle with semi-transparent white background
- Leader line connects box edge to the pointed location
- High readability even on complex images

### 8.3 `numbered`
```
    Image has: ① ② ③ (circled numbers on the diagram)
    
    Legend below image:
    ① Phalanges
    ② Metacarpals  
    ③ Carpals
```
- Compact numbering on the image
- Full legend below or beside
- Good for dense diagrams

### 8.4 `color_coded`
```
    ● Phalanges     (in red)
    ● Metacarpals   (in blue)
    ● Carpals       (in green)
```
- Each label gets a unique color
- Colored dot on the image matches the label color
- Optional: colored highlight/outline on the image region

### 8.5 `minimal`
```
    Phalanges ─── •
```
- Very thin lines, small text
- Low visual clutter
- Good for complex diagrams with many labels

### 8.6 `textbook`
```
    Phalanges ─┤
               │
    Metacarpals┤  (bracket-style grouping)
               │
    Carpals ───┘
```
- Classic textbook layout with brackets/braces
- Bold serif font
- Leader lines with right-angle bends

---

## 9. API Changes

### `/api/generate` — Modified

**New request body:**
```json
{
  "message": "draw a human hand anatomy",
  "session_id": null,
  "llm_provider": "gemini",
  "image_provider": "openai",
  "label_style": "boxed_text",
  "skip_annotate": false,
  "show_plan": false
}
```

**New response:**
```json
{
  "session_id": "abc123",
  "steps": [
    { "stage": "diagram_planner", "result": { "entities": 5, "auditor_iterations": 2 } },
    { "stage": "image_generator", "result": "/generated/abc123/raster.png" },
    { "stage": "label_renderer", "result": "/generated/abc123/annotated.png" },
    { "stage": "exporter", "result": "/generated/abc123/annotated.svg" }
  ],
  "raster_url": "/generated/abc123/raster.png",
  "annotated_url": "/generated/abc123/annotated.png",
  "svg_url": "/generated/abc123/annotated.svg",
  "plan": { ... },
  "spec": { ... }
}
```

**Removed fields:** `skip_vectorize` (no more vectorizer)

---

## 10. Risk Assessment & Mitigations

| Risk | Likelihood | Mitigation |
|------|-----------|------------|
| LLM generates bad bounding boxes (overlapping labels) | Medium | Auditor loop (1-3 iterations) catches most issues |
| DALL-E 3 ignores spatial hints in prompt | Medium | Spatial hints are "soft" — we position labels based on the plan regardless. If the image doesn't match, labels still appear in logical positions |
| Pillow text rendering looks low-quality | Low | Use antialiased TrueType fonts (Arial/Helvetica). Pillow's text rendering is production-quality |
| Plan format LLM hallucinations (invalid JSON) | Medium | Robust JSON parser with fallback. Few-shot examples guide format |
| User expects pixel-perfect vectorization | Low | The new approach produces annotated PNGs + hybrid SVGs (raster base + vector labels). Document that full vectorization is not included |

---

## 11. Dependencies Change

### Removed
- `pydiffvg` (compiled DiffVG)
- `scikit-fmm`
- `clip` (OpenAI CLIP)
- PyTorch-SVGRender as subprocess dependency

### Kept
- `Pillow` (now critical for label rendering)
- `google-genai`, `openai` (LLM + image gen APIs)
- `fastapi`, `uvicorn`, `jinja2` (web server)

### Added (optional)
- A TrueType font file (Arial/Helvetica) placed in `static/fonts/` for consistent label rendering across OS

---

## 12. Summary

The integration replaces the **slowest and most fragile part** of the pipeline (LIVE vectorizer, 30+ min) with DiagrammerGPT's **LLM-planned layout + programmatic text rendering** approach (<1 second). The result:

1. **LLM generates a precise diagram plan** with entity bounding boxes (DiagrammerGPT's core innovation)
2. **Auditor loop** catches layout errors before generation
3. **Image gen API** produces the visual (same as before, but with spatial hints)
4. **Pillow/SVG renders labels** programmatically — crisp, readable, instant
5. **User controls annotation style** via UI dropdown (6 preset styles)

Total pipeline time drops from **30+ minutes (failing)** to **~20-25 seconds (working)**.
